import requests
import pandas as pd
import time
import json
from datetime import date
from dateutil.relativedelta import relativedelta
import os
import glob # ì„ì‹œ íŒŒì¼ë“¤ì„ ì°¾ê¸° ìœ„í•´ ì¶”ê°€

# =================================================================
# âš ï¸ 1. í•„ìˆ˜ ì„¤ì • ê°’ (API ìŠ¹ì¸ í›„ ì—¬ê¸°ì— ì…ë ¥í•˜ì„¸ìš”!)
# =================================================================

# ğŸ”‘ ê³µê³µë°ì´í„° í¬í„¸ì—ì„œ ë°œê¸‰ë°›ì€ ì¸ì¦í‚¤ (ì‹¤ì œ í‚¤ë¡œ êµì²´ í•„ìš”)
SERVICE_KEY = "Cq480HPo0y5QFitQIm+UL3uFOjcVKGpcaDbZi9BX7EG58UgzgC+FSDerU8zGtdNzZSh+fKIIL354yDRejTw+vA=="

# ğŸ™ï¸ ìˆ˜ë„ê¶Œ ì£¼ìš” ASOS ê´€ì¸¡ ì§€ì  ë²ˆí˜¸ (Stn ID)
STN_IDS = ['108', '112', '119', '203', '99', '201'] 

# ğŸ—“ï¸ ì¡°íšŒ ê¸°ê°„ ì„¤ì •
START_YEAR = 2021
END_YEAR = 2024

# ğŸ“ íŒŒì¼ ê²½ë¡œ ì„¤ì •
DATA_DIR = 'data'
TEMP_FILE_PREFIX = 'temp_asos_chunk_'
FINAL_OUTPUT_FILE = os.path.join(DATA_DIR, 'asos_su-do-gwon_final.csv')

# =================================================================
# 2. API ì„¤ì • ë° ë°ì´í„° ìˆ˜ì§‘ í•¨ìˆ˜
# =================================================================

BASE_URL = "http://apis.data.go.kr/1360000/AsosHourlyInfoService/getWthrDataList"

COMMON_PARAMS = {
    'serviceKey': SERVICE_KEY,
    'dataCd': 'ASOS',
    'dateCd': 'HR',
    'dataType': 'JSON',
    'endHh': '23',
    'startHh': '00',
    'numOfRows': '50', # 504 Time-out ë°©ì§€ë¥¼ ìœ„í•´ ìš”ì²­ ê±´ìˆ˜ë¥¼ 50ìœ¼ë¡œ ë‚®ì¶¤
}

def fetch_asos_data(stn_id, start_date_str, end_date_str):
    """íŠ¹ì • ì§€ì ê³¼ ê¸°ê°„ì˜ ASOS ì‹œê°„ ìë£Œë¥¼ APIë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤. (Time-out ì‹œ ìë™ ì¬ì‹œë„ í¬í•¨)"""
    
    params = COMMON_PARAMS.copy()
    params.update({
        'stnIds': stn_id,
        'startDt': start_date_str,
        'endDt': end_date_str,
    })
    
    all_records = []
    page_no = 1
    MAX_RETRIES = 3 # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    
    while True:
        params['pageNo'] = str(page_no)
        response = None
        
        # --- Time-out ë° Status Code ì˜¤ë¥˜ ì‹œ ì¬ì‹œë„ ë£¨í”„ ---
        for attempt in range(MAX_RETRIES):
            try:
                # Time-out 90ì´ˆ ì„¤ì •
                response = requests.get(BASE_URL, params=params, timeout=90)
                
                if response.status_code != 200:
                    print(f"[{stn_id}, {start_date_str}, p{page_no}] ìƒíƒœ ì½”ë“œ {response.status_code}. ì¬ì‹œë„ ({attempt + 1}/{MAX_RETRIES})")
                    time.sleep(2 ** attempt * 2) 
                    continue
                
                break 
            except requests.exceptions.Timeout:
                print(f"[{stn_id}, {start_date_str}, p{page_no}] ìš”ì²­ ì‹œê°„ ì´ˆê³¼(Timeout). ì¬ì‹œë„ ({attempt + 1}/{MAX_RETRIES})")
                time.sleep(2 ** attempt * 3) 
                continue
            except Exception as e:
                print(f"[FATAL ERROR] ì˜ˆê¸°ì¹˜ ì•Šì€ ì—°ê²° ì˜¤ë¥˜ ë°œìƒ: {e}")
                return all_records
        
        if response is None or response.status_code != 200:
            return all_records

        # --- ë°ì´í„° ì²˜ë¦¬ ---
        try:
            data = response.json()
            response_body = data.get('response', {}).get('body')
            
            if not response_body:
                header_msg = data.get('response', {}).get('header', {}).get('resultMsg', 'Unknown Error')
                print(f"[FAIL] ë°ì´í„° ì—†ìŒ ë˜ëŠ” ì˜¤ë¥˜: {header_msg}")
                break
                
            items = response_body.get('items', {}).get('item')
            
            if not items:
                break
                
            all_records.extend(items)
            total_count = response_body.get('totalCount', 0)
            
            if len(all_records) >= total_count:
                break
                
            page_no += 1
            time.sleep(0.5) 
            
        except json.JSONDecodeError:
            print(f"[FAIL] JSON ë””ì½”ë”© ì˜¤ë¥˜ (ì‘ë‹µ ë³¸ë¬¸ í™•ì¸ í•„ìš”).")
            break
        except Exception as e:
            print(f"[FATAL ERROR] ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            break

    return all_records

# =================================================================
# 3. ë©”ì¸ ì‹¤í–‰ ë° ì €ì¥ (ë¶„ê¸°ë³„ ë°˜ë³µ ë° ì„ì‹œ ì €ì¥)
# =================================================================

if SERVICE_KEY == "YOUR_SERVICE_KEY_HERE":
    print("ğŸ”´ ì‹¤í–‰ ì‹¤íŒ¨: SERVICE_KEYë¥¼ ë°œê¸‰ë°›ì€ ì‹¤ì œ í‚¤ë¡œ êµì²´ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.")
else:
    # data í´ë” ì¡´ì¬ í™•ì¸ ë° ìƒì„±
    if not os.path.exists(DATA_DIR):
        os.makedirs(DATA_DIR)

    current_date = date(START_YEAR, 1, 1)
    end_date_limit = date(END_YEAR + 1, 1, 1) # ë‹¤ìŒ í•´ 1ì›” 1ì¼ ì§ì „ê¹Œì§€

    print("--- ê¸°ìƒì²­ ASOS ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ (ë¶„ê¸°ë³„ ë¶„í•  ìš”ì²­) ---")

    while current_date < end_date_limit:
        next_quarter = current_date + relativedelta(months=3)
        
        start_dt_str = current_date.strftime("%Y%m%d")
        # ì¢…ë£Œì¼ì€ ë‹¤ìŒ ë¶„ê¸°ì˜ ì „ë‚ ë¡œ ì„¤ì •
        end_dt_str = (next_quarter - relativedelta(days=1)).strftime("%Y%m%d")

        # ì„ì‹œ íŒŒì¼ëª… ì •ì˜
        temp_file_name = f"{TEMP_FILE_PREFIX}{start_dt_str}_{end_dt_str}.csv"
        temp_file_path = os.path.join(DATA_DIR, temp_file_name)
        
        # í•´ë‹¹ ë¶„ê¸°ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì„ì‹œ íŒŒì¼ ì €ì¥ í›„ ì´ˆê¸°í™”ë¨)
        quarter_records = []

        print(f"\n[ë¶„ê¸° ì‹œì‘] {start_dt_str} ~ {end_dt_str}")

        # --- ë¶„ê¸°ë³„ API í˜¸ì¶œ ë£¨í”„ ---
        for stn_id in STN_IDS:
            print(f"-> ìˆ˜ì§‘ ì¤‘: ì§€ì  {stn_id}")
            records = fetch_asos_data(stn_id, start_dt_str, end_dt_str)
            
            if records:
                quarter_records.extend(records)
                print(f"   [SUCCESS] {len(records):,} ê±´ ìˆ˜ì§‘ ì™„ë£Œ.")
            else:
                print(f"   [FAIL] ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ.")
            
            time.sleep(1) # API í˜¸ì¶œ ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ì•ˆì „ ëŒ€ê¸° ì‹œê°„
        # ----------------------------
        
        if quarter_records:
            # --- ë¶„ê¸°ë³„ ë°ì´í„° ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ ---
            df_quarter = pd.DataFrame(quarter_records)
            
            # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° (ì¤‘ë‹¨ í›„ ì¬ì‹œì‘), í—¤ë” ì—†ì´ ì´ì–´ë¶™ì„
            write_header = not os.path.exists(temp_file_path)
            
            df_quarter.to_csv(temp_file_path, index=False, encoding='utf-8-sig', header=write_header)
            
            print(f"\nğŸ’¾ ë¶„ê¸° ë°ì´í„° ì„ì‹œ ì €ì¥ ì™„ë£Œ: {temp_file_path} (ì´ {len(quarter_records):,} ê±´)")
        
        # ë‹¤ìŒ ë¶„ê¸°ë¡œ ì´ë™
        current_date = next_quarter

    # =================================================================
    # 4. ìµœì¢… íŒŒì¼ í•©ë³¸ ìƒì„± ë° ì„ì‹œ íŒŒì¼ ì‚­ì œ
    # =================================================================
    print("\n\n---  ìµœì¢… íŒŒì¼ í•©ë³¸ ìƒì„± ì‹œì‘ ---")
    
    all_temp_files = glob.glob(os.path.join(DATA_DIR, f'{TEMP_FILE_PREFIX}*.csv'))
    
    if all_temp_files:
        combined_df = []
        
        # ì²« ë²ˆì§¸ íŒŒì¼ë§Œ í—¤ë”ë¥¼ í¬í•¨í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ë°ì´í„°ë§Œ ì½ìŒ
        for i, file_path in enumerate(all_temp_files):
            df_chunk = pd.read_csv(file_path, encoding='utf-8-sig')
            combined_df.append(df_chunk)

        df_final_weather = pd.concat(combined_df, ignore_index=True)
        
        # ìµœì¢… íŒŒì¼ ì €ì¥
        df_final_weather.to_csv(FINAL_OUTPUT_FILE, index=False, encoding='utf-8-sig')
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        #for file_path in all_temp_files:
             #os.remove(file_path)
        
        print("\n=============================================")
        print(f"âœ… ìµœì¢… ê¸°ìƒ ë°ì´í„° í•©ë³¸ ì™„ë£Œ! ì´ {len(df_final_weather):,} ê±´.")
        print(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ê²½ë¡œ: {FINAL_OUTPUT_FILE}")
        print("ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ëª¨ë‘ ì‚­ì œ ì™„ë£Œ.")
        print("=============================================")
    else:
        print("\nğŸ”´ ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")